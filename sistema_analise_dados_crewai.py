"""
Sistema de Agentes CrewAI para AnÃ¡lise de Bases de Dados com Google Gemini

VERSÃƒO CORRIGIDA - Problema de file_path vazio resolvido + SUPORTE A ZIP DE NFs

"""

import os
import pandas as pd
import json
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple
from crewai import Agent, Task, Crew, LLM
from crewai.tools import BaseTool
from pydantic import BaseModel, Field
import chardet
import unicodedata
import re
import textwrap
import tempfile
import zipfile
import shutil
from dotenv import load_dotenv

# Carregar variÃ¡veis de ambiente
load_dotenv()

# =============================================================================
# PROCESSADOR DE DADOS UNIFICADO (MODIFICADO PARA SUPORTE A ZIP)
# =============================================================================

class BaseDataProcessor:
    """Processador centralizado que garante dados idÃªnticos entre agentes + Suporte ZIP"""
    
    _cache_dataframes = {}
    _current_file_path = None
    
    @classmethod
    def get_processed_dataframe(cls, file_path: str) -> pd.DataFrame:
        abs_path = os.path.abspath(file_path)
        cls._current_file_path = abs_path
        
        if abs_path not in cls._cache_dataframes:
            print(f"ğŸ”„ Processando dados pela primeira vez: {file_path}")
            
            # NOVO: Verificar se Ã© arquivo ZIP
            if file_path.lower().endswith('.zip'):
                df = cls._process_zip_nfs(file_path)
            else:
                df = cls._process_file_unified(file_path)
            
            cls._cache_dataframes[abs_path] = df
            print(f"âœ… Dados processados e armazenados no cache: {len(df)} registros, {len(df.columns)} colunas")
            print(f"ğŸ“‹ Colunas: {list(df.columns)}")
        else:
            print(f"ğŸ“¦ Usando dados do cache: {file_path}")
            df = cls._cache_dataframes[abs_path]
        
        return df.copy()
    
    @classmethod
    def get_current_file_path(cls) -> str:
        """Retorna o file_path atual em uso"""
        return cls._current_file_path or ""
    
    @classmethod
    def _process_zip_nfs(cls, zip_path: str) -> pd.DataFrame:
        """NOVO: Processar arquivo ZIP contendo CSVs de Notas Fiscais"""
        try:
            print(f"ğŸ“¦ Processando arquivo ZIP de NFs: {zip_path}")
            
            with tempfile.TemporaryDirectory() as temp_dir:
                # Extrair ZIP
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall(temp_dir)
                
                # Procurar arquivos CSV especÃ­ficos
                extracted_files = os.listdir(temp_dir)
                cabecalho_file = None
                itens_file = None
                
                for file in extracted_files:
                    if file.endswith('_NFs_Cabecalho.csv'):
                        cabecalho_file = os.path.join(temp_dir, file)
                        print(f"ğŸ“„ Encontrado arquivo CabeÃ§alho: {file}")
                    elif file.endswith('_NFs_Itens.csv'):
                        itens_file = os.path.join(temp_dir, file)
                        print(f"ğŸ“„ Encontrado arquivo Itens: {file}")
                
                if not cabecalho_file or not itens_file:
                    raise ValueError(
                        "Arquivo ZIP deve conter exatamente dois arquivos: "
                        "aaaamm_NFs_Cabecalho.csv e aaaamm_NFs_Itens.csv"
                    )
                
                # Processar cada arquivo CSV
                print("ğŸ”§ Processando arquivo CabeÃ§alho...")
                df_cabecalho = cls._process_file_unified(cabecalho_file)
                
                print("ğŸ”§ Processando arquivo Itens...")
                df_itens = cls._process_file_unified(itens_file)
                
                # Mergear baseado na CHAVE DE ACESSO
                print("ğŸ”— Realizando merge baseado na CHAVE DE ACESSO...")
                
                # Normalizar nome da coluna chave em ambos DataFrames
                chave_col_cabecalho = cls._encontrar_coluna_chave(df_cabecalho.columns)
                chave_col_itens = cls._encontrar_coluna_chave(df_itens.columns)
                
                if not chave_col_cabecalho or not chave_col_itens:
                    raise ValueError("NÃ£o foi possÃ­vel encontrar a coluna 'CHAVE DE ACESSO' em um dos arquivos")
                
                # Realizar merge
                df_merged = pd.merge(
                    df_cabecalho, 
                    df_itens, 
                    left_on=chave_col_cabecalho,
                    right_on=chave_col_itens,
                    how='inner',
                    suffixes=('_CABECALHO', '_ITENS')
                )
                
                print(f"âœ… Merge realizado com sucesso: {len(df_merged)} registros finais")
                print(f"ğŸ“Š EstatÃ­sticas do merge:")
                print(f"   - Registros CabeÃ§alho: {len(df_cabecalho)}")
                print(f"   - Registros Itens: {len(df_itens)}")
                print(f"   - Registros Merged: {len(df_merged)}")
                
                # Adicionar metadados especÃ­ficos do ZIP
                df_merged.attrs['tipo_processamento'] = 'ZIP_NFs'
                df_merged.attrs['arquivo_cabecalho'] = os.path.basename(cabecalho_file)
                df_merged.attrs['arquivo_itens'] = os.path.basename(itens_file)
                df_merged.attrs['coluna_merge'] = chave_col_cabecalho
                df_merged.attrs['arquivo_origem'] = zip_path
                
                return df_merged
                
        except Exception as e:
            print(f"âŒ Erro ao processar ZIP de NFs: {e}")
            raise e
    
    @classmethod
    def _encontrar_coluna_chave(cls, colunas: List[str]) -> Optional[str]:
        """Encontrar a coluna CHAVE DE ACESSO considerando normalizaÃ§Ãµes"""
        for col in colunas:
            col_normalizada = col.upper().replace(' ', '').replace('_', '')
            if 'CHAVEDEACESSO' in col_normalizada or 'CHAVEACESSO' in col_normalizada:
                return col
        return None
    
    @classmethod
    def _process_file_unified(cls, file_path: str) -> pd.DataFrame:
        try:
            encoding = cls._detectar_encoding_robusto(file_path)
            print(f"ğŸ” Encoding detectado: {encoding}")
            
            df = pd.read_csv(file_path, encoding=encoding)
            print(f"ğŸ“Š Arquivo lido: {len(df)} registros, {len(df.columns)} colunas")
            
            colunas_originais = list(df.columns)
            df = cls._normalizar_nomes_colunas(df)
            print(f"ğŸ”§ Colunas normalizadas: {list(df.columns)}")
            
            df.attrs['colunas_originais'] = colunas_originais
            df.attrs['encoding_usado'] = encoding
            df.attrs['arquivo_origem'] = file_path
            
            return df
            
        except Exception as e:
            print(f"âŒ Erro no processamento unificado: {e}")
            raise e
    
    @classmethod
    def _detectar_encoding_robusto(cls, file_path: str) -> str:
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read()
            result = chardet.detect(raw_data)
            detected_encoding = result['encoding']
            
            encodings_comuns = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1', 'windows-1252']
            
            for encoding in encodings_comuns:
                try:
                    df_test = pd.read_csv(file_path, encoding=encoding, nrows=5)
                    problemas = sum(1 for col in df_test.columns 
                                  if any(char in col for char in ['Ãƒ', 'Ã‚', 'Ã‡', 'Â¿', 'Â½', 'âˆš', 'âˆ«']))
                    if problemas == 0:
                        return encoding
                except:
                    continue
            
            return detected_encoding or 'latin-1'
        except Exception:
            return 'latin-1'
    
    @classmethod
    def _normalizar_nomes_colunas(cls, df: pd.DataFrame) -> pd.DataFrame:
        colunas_normalizadas = []
        for col in df.columns:
            try:
                sem_acentos = unicodedata.normalize('NFD', col).encode('ascii', 'ignore').decode('utf-8')
            except:
                sem_acentos = col
            
            limpo = re.sub(r'[^\w\s-]', '', sem_acentos)
            normalizado = ' '.join(limpo.split())
            colunas_normalizadas.append(normalizado)
        
        df.columns = colunas_normalizadas
        return df
    
    @classmethod
    def clear_cache(cls):
        cls._cache_dataframes.clear()
        cls._current_file_path = None
        print("ğŸ—‘ï¸ Cache de DataFrames limpo")

# =============================================================================
# CONFIGURAÃ‡ÃƒO DO MODELO GEMINI
# =============================================================================

def configurar_gemini():
    # Remover aspas ou espaÃ§os que possam estar na chave
    gemini_api_key = os.getenv("GEMINI_API_KEY").strip().replace('"', '')
    if not gemini_api_key:
        raise ValueError("GEMINI_API_KEY nÃ£o encontrada nas variÃ¡veis de ambiente")

    gemini_llm = LLM(
        model='gemini/gemini-1.5-flash',
        api_key=gemini_api_key,
        temperature=0.5
    )
    return gemini_llm

# =============================================================================
# FERRAMENTAS PERSONALIZADAS - VERSÃƒO COM CORREÃ‡ÃƒO ESPECÃFICA
# =============================================================================

class DataAnalysisInput(BaseModel):
    file_path: str = Field(description="Caminho para o arquivo CSV")
    query: str = Field(description="Consulta para anÃ¡lise especÃ­fica", default="")

class DataAnalysisTool(BaseTool):
    name: str = "data_analysis_tool"
    description: str = "Ferramenta para anÃ¡lise robusta de dados CSV com processamento unificado"
    args_schema: type[BaseModel] = DataAnalysisInput

    def _run(self, file_path: str, query: str = "") -> dict:
        try:
            print(f"ğŸ” DataAnalysisTool - Analisando: {file_path}")
            df = BaseDataProcessor.get_processed_dataframe(file_path)

            print(f"ğŸ“Š DataAnalysisTool - DataFrame carregado: {len(df)} x {len(df.columns)}")
            print(f"ğŸ“‹ DataAnalysisTool - Colunas: {list(df.columns)}")

            info_basica = {
                "shape": list(df.shape),
                "columns": list(df.columns),
                "dtypes": df.dtypes.to_dict(),
                "null_values": df.isnull().sum().to_dict(),
                "encoding_usado": df.attrs.get('encoding_usado', 'utf-8')
            }

            numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
            numeric_stats = {}
            for col in numeric_cols:
                try:
                    stats = df[col].describe()
                    numeric_stats[col] = stats.to_dict()
                except:
                    pass
            info_basica["numeric_stats"] = numeric_stats

            unique_counts = {}
            for col in df.columns:
                try:
                    unique_counts[col] = df[col].nunique()
                except:
                    unique_counts[col] = 0
            info_basica["unique_values"] = unique_counts

            try:
                sample_data = df.head(3).to_dict('records')
                info_basica["sample_data"] = sample_data
            except:
                info_basica["sample_data"] = []

            colunas_mapeamento = {}
            if 'colunas_originais' in df.attrs:
                for i, col_orig in enumerate(df.attrs['colunas_originais']):
                    if i < len(df.columns):
                        colunas_mapeamento[col_orig] = df.columns[i]
            info_basica["mapeamento_colunas"] = colunas_mapeamento

            return info_basica

        except Exception as e:
            return {
                "erro": f"Erro ao analisar arquivo: {str(e)}",
                "sugestao": "Verifique se o arquivo existe e estÃ¡ em formato CSV vÃ¡lido"
            }

class PythonExecutorInput(BaseModel):
    code: str = Field(description="CÃ³digo Python para executar")
    file_path: str = Field(description="Caminho para o arquivo de dados")

class PythonExecutorTool(BaseTool):
    name: str = "python_executor_tool"
    description: str = "Executa cÃ³digo Python usando dados processados de forma unificada - CORREÃ‡ÃƒO file_path"
    args_schema: type[BaseModel] = PythonExecutorInput

    def _preparar_ambiente_execucao(self, file_path: str) -> Tuple[Dict, Optional[str]]:
        """CORREÃ‡ÃƒO PRINCIPAL: Usar file_path do cache se vazio"""
        try:
            # CORREÃ‡ÃƒO: Se file_path estÃ¡ vazio, usar o do cache
            if not file_path or file_path.strip() == "":
                file_path = BaseDataProcessor.get_current_file_path()
                print(f"ğŸ”§ Usando file_path do cache: {file_path}")

            if not file_path:
                return {}, "Erro: Nenhum arquivo de dados disponÃ­vel"

            print(f"ğŸ PythonExecutorTool - Preparando ambiente: {file_path}")

            # USAR PROCESSAMENTO UNIFICADO
            df = BaseDataProcessor.get_processed_dataframe(file_path)

            print(f"ğŸ“Š PythonExecutorTool - DataFrame carregado: {len(df)} x {len(df.columns)}")
            print(f"ğŸ“‹ PythonExecutorTool - Colunas: {list(df.columns)}")

            # Preparar namespace completo
            namespace = {
                'pd': pd,
                'df': df,
                'file_path': file_path,
                'encoding_usado': df.attrs.get('encoding_usado', 'utf-8'),
                'os': os,
                're': re,
                'unicodedata': unicodedata,
                'result': None,
                'colunas_disponiveis': list(df.columns),
                'shape': df.shape,
                'print': print
            }

            return namespace, None

        except Exception as e:
            print(f"âŒ Erro ao preparar ambiente: {e}")
            return {}, str(e)

    def _run(self, code: str, file_path: str) -> str:
        """MÃ‰TODO PRINCIPAL CORRIGIDO: Tratamento de file_path vazio"""
        try:
            print(f"ğŸ PythonExecutorTool - Executando cÃ³digo para: {file_path}")

            # Preparar ambiente com correÃ§Ã£o de file_path
            namespace, erro = self._preparar_ambiente_execucao(file_path)
            if erro:
                return f"Erro ao carregar dados: {erro}"

            # Processar e executar cÃ³digo
            codigo_processado = self._processar_codigo_json(code)

            print(f"ğŸ”§ Executando cÃ³digo processado...")

            # Executar cÃ³digo
            try:
                exec(codigo_processado, namespace)
                print("âœ… CÃ³digo executado com sucesso")
            except Exception as e:
                print(f"âŒ Erro na execuÃ§Ã£o: {e}")
                return self._executar_fallback_direto(namespace, f"Erro na execuÃ§Ã£o: {e}")

            # Capturar resultado
            if 'result' in namespace and namespace['result'] is not None:
                resultado = namespace['result']

                if hasattr(resultado, 'to_string'):
                    return resultado.to_string()
                elif hasattr(resultado, 'to_dict'):
                    return str(resultado.to_dict())
                else:
                    return str(resultado)
            else:
                return "CÃ³digo executado com sucesso. Verifique se o resultado foi atribuÃ­do Ã  variÃ¡vel 'result'."

        except Exception as e:
            return self._executar_fallback_direto(namespace if 'namespace' in locals() else {}, f"Erro geral: {str(e)}")

    def _processar_codigo_json(self, code_input: str) -> str:
        """Processar cÃ³digo JSON com correÃ§Ã£o de encoding"""
        try:
            if code_input.startswith('{') and '"code"' in code_input:
                try:
                    data = json.loads(code_input)
                    codigo_bruto = data.get('code', code_input)
                except json.JSONDecodeError:
                    codigo_bruto = code_input
            else:
                codigo_bruto = code_input

            # Processar sequÃªncias de escape
            codigo_processado = codigo_bruto.replace('\\n', '\n')
            codigo_processado = codigo_processado.replace('\\\\', '\\')

            # Processar caracteres Unicode
            try:
                codigo_processado = codigo_processado.encode().decode('unicode_escape')
            except UnicodeDecodeError:
                codigo_processado = codigo_processado.replace('\\u00e3', 'Ã£')
                codigo_processado = codigo_processado.replace('\\u00e7', 'Ã§')

            # Normalizar indentaÃ§Ã£o
            codigo_final = textwrap.dedent(codigo_processado)

            return codigo_final

        except Exception as e:
            print(f"âŒ Erro ao processar cÃ³digo JSON: {e}")
            return code_input

    def _executar_fallback_direto(self, namespace: Dict, erro_original: str) -> str:
        """ExecuÃ§Ã£o de fallback usando DataFrame jÃ¡ carregado"""
        try:
            print(f"ğŸ”„ Executando fallback direto...")

            if 'df' not in namespace:
                return f"Erro original: {erro_original}\nDataFrame nÃ£o disponÃ­vel para fallback."

            df = namespace['df']

            # Fallback especÃ­fico para perguntas sobre fornecedores/valores
            if 'NOME DESTINATARIO' in df.columns and 'VALOR NOTA FISCAL' in df.columns:
                print("ğŸ” Executando anÃ¡lise de fallback para fornecedores...")
                df['VALOR NOTA FISCAL'] = pd.to_numeric(df['VALOR NOTA FISCAL'], errors='coerce')
                resultado = df.groupby('NOME DESTINATARIO')['VALOR NOTA FISCAL'].sum().sort_values(ascending=False)
                top_fornecedor = resultado.index[0]
                valor_total = resultado.iloc[0]

                return f"Fornecedor com maior valor total: {top_fornecedor} - R$ {valor_total:,.2f}\n\nTop 5 fornecedores:\n{resultado.head().to_string()}"

            # Fallback para emitentes por CNPJ
            elif 'CPFCNPJ Emitente' in df.columns and 'VALOR NOTA FISCAL' in df.columns:
                print("ğŸ” Executando anÃ¡lise de fallback para emitentes por CNPJ...")
                df['VALOR NOTA FISCAL'] = pd.to_numeric(df['VALOR NOTA FISCAL'], errors='coerce')
                resultado = df.groupby('CPFCNPJ Emitente')['VALOR NOTA FISCAL'].sum().sort_values(ascending=False)
                top_emitente_cnpj = resultado.index[0]
                valor_total = resultado.iloc[0]

                # Tentar encontrar o nome do emitente
                nome_emitente = "NÃ£o informado"
                if 'RAZAO SOCIAL EMITENTE' in df.columns:
                    mask = df['CPFCNPJ Emitente'] == top_emitente_cnpj
                    nomes = df.loc[mask, 'RAZAO SOCIAL EMITENTE'].unique()
                    if len(nomes) > 0:
                        nome_emitente = nomes[0]

                return f"Fornecedor com maior valor total: {nome_emitente} (CNPJ: {top_emitente_cnpj}) - R$ {valor_total:,.2f}\n\nTop 5 fornecedores por CNPJ:\n{resultado.head().to_string()}"

            # Fallback genÃ©rico
            colunas_disponiveis = list(df.columns)
            return f"Erro original: {erro_original}\nColunas disponÃ­veis: {colunas_disponiveis}\nShape: {df.shape}"

        except Exception as e2:
            return f"Erro original: {erro_original}\nErro no fallback: {str(e2)}"

# =============================================================================
# AGENTES (MANTIDOS COM PEQUENOS AJUSTES)
# =============================================================================

def criar_agente_metadados(llm):
    return Agent(
        role='Especialista em Metadados de Dados',
        goal='Gerar metadados completos e robustos para bases de dados CSV',
        backstory="""VocÃª Ã© um especialista em anÃ¡lise de dados com vasta experiÃªncia em
        caracterizaÃ§Ã£o de datasets. Sua especialidade Ã© extrair e documentar metadados
        detalhados de bases de dados, incluindo tipos de dados, distribuiÃ§Ãµes,
        relacionamentos, qualidade dos dados e padrÃµes identificados.""",
        verbose=True,
        allow_delegation=False,
        tools=[DataAnalysisTool()],
        llm=llm
    )

def criar_agente_codigo(llm):
    return Agent(
        role='Especialista em CÃ³digo Python para AnÃ¡lise de Dados',
        goal='Gerar cÃ³digo Python eficiente para responder perguntas especÃ­ficas sobre dados',
        backstory="""VocÃª Ã© um programador Python especializado em anÃ¡lise de dados
        com pandas, numpy e outras bibliotecas. VocÃª recebe metadados de uma base de dados
        e uma pergunta do usuÃ¡rio, e gera cÃ³digo Python preciso e eficiente para
        responder Ã  pergunta. IMPORTANTE: Use sempre o DataFrame 'df' que jÃ¡ estÃ¡
        carregado no ambiente. O arquivo de dados serÃ¡ carregado automaticamente.
        Sempre atribua o resultado final Ã  variÃ¡vel result.""",
        verbose=True,
        allow_delegation=False,
        tools=[PythonExecutorTool()],
        llm=llm
    )

def criar_agente_linguagem_natural(llm):
    return Agent(
        role='Especialista em ComunicaÃ§Ã£o de Dados',
        goal='Converter resultados tÃ©cnicos em respostas claras em linguagem natural',
        backstory="""VocÃª Ã© um comunicador especializado em traduzir resultados
        tÃ©cnicos e numÃ©ricos em explicaÃ§Ãµes claras e compreensÃ­veis para usuÃ¡rios
        nÃ£o tÃ©cnicos. VocÃª recebe cÃ³digos Python e seus resultados e cria respostas
        em linguagem natural que sÃ£o informativas, precisas e fÃ¡ceis de entender.""",
        verbose=True,
        allow_delegation=False,
        llm=llm
    )

# =============================================================================
# TAREFAS (CORREÃ‡ÃƒO ESPECÃFICA)
# =============================================================================

def criar_tarefa_metadados(agente, file_path):
    return Task(
        description=f"""Analise a base de dados localizada em {file_path} e gere
        metadados completos e robustos. Os metadados devem incluir:

        1. InformaÃ§Ãµes bÃ¡sicas: nÃºmero de linhas, colunas, tamanho
        2. DescriÃ§Ã£o detalhada de cada coluna: tipo, valores Ãºnicos e padrÃµes
        3. EstatÃ­sticas descritivas para colunas numÃ©ricas
        4. IdentificaÃ§Ã£o de valores nulos ou inconsistÃªncias

        Use a ferramenta data_analysis_tool para extrair as informaÃ§Ãµes necessÃ¡rias.""",
        expected_output="""Um relatÃ³rio detalhado de metadados contendo:
        - Estrutura da base de dados
        - DescriÃ§Ã£o de cada campo/coluna
        - EstatÃ­sticas e distribuiÃ§Ãµes
        - Qualidade dos dados""",
        agent=agente
    )

def criar_tarefa_codigo(agente, user_query, metadata, file_path):
    return Task(
        description=f"""Com base nos metadados fornecidos e na pergunta do usuÃ¡rio,
        gere cÃ³digo Python usando pandas para responder Ã  pergunta:

        PERGUNTA DO USUÃRIO: {user_query}

        METADADOS DA BASE DE DADOS: {metadata}

        ARQUIVO DE DADOS: {file_path}

        INSTRUÃ‡Ã•ES CRÃTICAS:
        1. O DataFrame 'df' JÃ ESTÃ CARREGADO no ambiente de execuÃ§Ã£o
        2. NÃƒO tente carregar arquivos CSV com pd.read_csv()
        3. Use APENAS o DataFrame 'df' disponÃ­vel
        4. SEMPRE atribua o resultado final Ã  variÃ¡vel result
        5. Inclua comentÃ¡rios explicativos no cÃ³digo
        6. FaÃ§a agregaÃ§Ãµes, filtragens ou cÃ¡lculos diretamente no 'df'
        7. O sistema carregarÃ¡ automaticamente os dados do arquivo: {file_path}

        EXEMPLO DE CÃ“DIGO CORRETO:
        # O DataFrame 'df' jÃ¡ estÃ¡ carregado
        # Agrupa por coluna e soma valores
        resultado = df.groupby('COLUNA')['VALOR'].sum()
        result = resultado.to_string()

        Use a ferramenta python_executor_tool para executar e testar o cÃ³digo.""",
        expected_output="""CÃ³digo Python funcional que:
        - Usa o DataFrame 'df' jÃ¡ carregado
        - Processa os dados conforme a pergunta
        - Atribui o resultado Ã  variÃ¡vel result
        - Inclui comentÃ¡rios explicativos
        - Retorna o resultado da execuÃ§Ã£o""",
        agent=agente
    )

def criar_tarefa_linguagem_natural(agente, user_query, codigo_resultado):
    return Task(
        description=f"""Converta o resultado tÃ©cnico em uma resposta clara
        em linguagem natural para a pergunta do usuÃ¡rio.

        PERGUNTA DO USUÃRIO: {user_query}

        CÃ“DIGO E RESULTADO TÃ‰CNICO: {codigo_resultado}

        InstruÃ§Ãµes:
        1. ForneÃ§a uma resposta direta e clara Ã  pergunta
        2. Explique o resultado em termos simples
        3. Inclua nÃºmeros relevantes ou estatÃ­sticas quando apropriado
        4. Contextualize o resultado se necessÃ¡rio
        5. Use linguagem acessÃ­vel para usuÃ¡rios nÃ£o tÃ©cnicos
        6. Se houver limitaÃ§Ãµes ou consideraÃ§Ãµes importantes, mencione-as""",
        expected_output="""Uma resposta em linguagem natural que:
        - Responde diretamente Ã  pergunta do usuÃ¡rio
        - Explica os resultados de forma clara
        - Inclui dados relevantes formatados de forma legÃ­vel
        - Fornece contexto quando necessÃ¡rio
        - Ã‰ compreensÃ­vel para usuÃ¡rios nÃ£o tÃ©cnicos""",
        agent=agente
    )

# =============================================================================
# CLASSE PRINCIPAL (CORREÃ‡ÃƒO ESPECÃFICA)
# =============================================================================

class SistemaAnaliseBaseDados:
    """Sistema principal com correÃ§Ã£o para file_path vazio"""

    def __init__(self):
        self.llm = configurar_gemini()
        self.agente_metadados = criar_agente_metadados(self.llm)
        self.agente_codigo = criar_agente_codigo(self.llm)
        self.agente_linguagem_natural = criar_agente_linguagem_natural(self.llm)
        self.metadados_cache = {}
        self.current_file_path = None  # NOVO: Manter referÃªncia do file_path atual

    # def gerar_metadados(self, file_path: str) -> str:
    #     if file_path in self.metadados_cache:
    #         return self.metadados_cache[file_path]

    #     print(f"ğŸ” Gerando metadados para: {file_path}")
    #     self.current_file_path = file_path  # NOVO: Salvar file_path atual

    #     # Garantir que dados estÃ£o processados
    #     BaseDataProcessor.get_processed_dataframe(file_path)

    #     tarefa_metadados = criar_tarefa_metadados(self.agente_metadados, file_path)
    #     crew_metadados = Crew(
    #         agents=[self.agente_metadados],
    #         tasks=[tarefa_metadados],
    #         verbose=True
    #     )

    #     resultado = crew_metadados.kickoff()
    #     self.metadados_cache[file_path] = str(resultado)
    #     return str(resultado)

    def gerar_metadados(self, file_path: str) -> dict:
        """MÃ‰TODO CORRIGIDO: Retorna dict ao invÃ©s de str para evitar erro de JSON"""
        if file_path in self.metadados_cache:
            return self.metadados_cache[file_path]

        print(f"ğŸ” Gerando metadados para: {file_path}")
        self.current_file_path = file_path

        # Garantir que dados estÃ£o processados
        df = BaseDataProcessor.get_processed_dataframe(file_path)
        
        # Gerar metadados estruturados diretamente
        metadados = {
            "informacoes_basicas": {
                "total_registros": len(df),
                "total_colunas": len(df.columns),
                "encoding_usado": df.attrs.get('encoding_usado', 'utf-8'),
                "tipo_processamento": df.attrs.get('tipo_processamento', 'CSV'),
                "arquivo_origem": df.attrs.get('arquivo_origem', file_path)
            },
            "colunas": {
                col: {
                    "tipo": str(df[col].dtype),
                    "valores_unicos": int(df[col].nunique()),
                    "valores_nulos": int(df[col].isnull().sum()),
                    "porcentagem_nulos": round(float(df[col].isnull().sum() / len(df) * 100), 2)
                } for col in df.columns
            },
            "estatisticas_numericas": {},
            "amostras": df.head(3).to_dict('records')
        }
        
        # Adicionar estatÃ­sticas para colunas numÃ©ricas
        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
        for col in numeric_cols:
            try:
                stats = df[col].describe()
                metadados["estatisticas_numericas"][col] = {
                    "media": float(stats['mean']),
                    "mediana": float(stats['50%']),
                    "desvio_padrao": float(stats['std']),
                    "minimo": float(stats['min']),
                    "maximo": float(stats['max']),
                    "q25": float(stats['25%']),
                    "q75": float(stats['75%'])
                }
            except:
                pass
        
        # InformaÃ§Ãµes especÃ­ficas do ZIP se aplicÃ¡vel
        if df.attrs.get('tipo_processamento') == 'ZIP_NFs':
            metadados["informacoes_zip"] = {
                "arquivo_cabecalho": df.attrs.get('arquivo_cabecalho', ''),
                "arquivo_itens": df.attrs.get('arquivo_itens', ''),
                "coluna_merge": df.attrs.get('coluna_merge', '')
            }
        
        self.metadados_cache[file_path] = metadados
        return metadados


    def responder_pergunta(self, file_path: str, user_query: str) -> str:
        print(f"â“ Pergunta: {user_query}")
        print(f"ğŸ“Š Base de dados: {file_path}")

        try:
            # CORREÃ‡ÃƒO: Garantir que file_path estÃ¡ disponÃ­vel globalmente
            self.current_file_path = file_path
            BaseDataProcessor.clear_cache()

            # Passo 1: Gerar/obter metadados
            metadados = self.gerar_metadados(file_path)

            # Passo 2: Gerar cÃ³digo Python - CORREÃ‡ÃƒO: Passar file_path explicitamente
            print("ğŸ Gerando cÃ³digo Python...")
            tarefa_codigo = criar_tarefa_codigo(
                self.agente_codigo, user_query, metadados, file_path  # file_path explÃ­cito
            )

            crew_codigo = Crew(
                agents=[self.agente_codigo],
                tasks=[tarefa_codigo],
                verbose=True
            )

            codigo_resultado = crew_codigo.kickoff()

            # Passo 3: Converter para linguagem natural
            print("ğŸ“ Convertendo para linguagem natural...")
            tarefa_linguagem = criar_tarefa_linguagem_natural(
                self.agente_linguagem_natural, user_query, str(codigo_resultado)
            )

            crew_linguagem = Crew(
                agents=[self.agente_linguagem_natural],
                tasks=[tarefa_linguagem],
                verbose=True
            )

            resposta_final = crew_linguagem.kickoff()
            return str(resposta_final)

        except Exception as e:
            print(f"âŒ Erro no processamento: {e}")
            return f"Erro ao processar pergunta: {str(e)}"

    def limpar_cache(self):
        self.metadados_cache.clear()
        BaseDataProcessor.clear_cache()
        self.current_file_path = None  # NOVO: Limpar file_path tambÃ©m
        print("ğŸ—‘ï¸ Todos os caches limpos")

# =============================================================================
# FUNÃ‡ÃƒO PRINCIPAL (MANTIDA INALTERADA)
# =============================================================================

def main():
    try:
        sistema = SistemaAnaliseBaseDados()

        file_path = "notas_fiscais.csv"

        perguntas_exemplo = [
            "Qual fornecedor teve o maior valor total recebido?",
            "Quantas notas fiscais foram emitidas por cada estado?",
            "Qual Ã© a mÃ©dia dos valores das notas fiscais?",
            "Quais sÃ£o os 5 maiores valores de nota fiscal?",
            "Quantas notas fiscais sÃ£o operaÃ§Ãµes interestaduais vs internas?"
        ]

        print("ğŸš€ Sistema de AnÃ¡lise de Base de Dados inicializado!")
        print("Exemplos de perguntas que vocÃª pode fazer:")
        for i, pergunta in enumerate(perguntas_exemplo, 1):
            print(f"{i}. {pergunta}")

        while True:
            print("=" * 60)
            pergunta = input("ğŸ’¬ Digite sua pergunta (ou 'quit' para sair): ").strip()

            if pergunta.lower() in ['quit', 'sair', 'exit']:
                print("ğŸ‘‹ Encerrando sistema...")
                break

            if not pergunta:
                print("âš ï¸ Por favor, digite uma pergunta vÃ¡lida.")
                continue

            try:
                resposta = sistema.responder_pergunta(file_path, pergunta)
                print(f"âœ… RESPOSTA:")
                print(resposta)
            except Exception as e:
                print(f"âŒ Erro ao processar pergunta: {str(e)}")

    except Exception as e:
        print(f"âŒ Erro ao inicializar sistema: {str(e)}")
        print("ğŸ’¡ Dicas:")
        print("1. Certifique-se de ter configurado a GEMINI_API_KEY")
        print("2. Instale as dependÃªncias: pip install crewai pandas google-generativeai")
        print("3. Verifique se o arquivo CSV existe no caminho especificado")

if __name__ == "__main__":
    main()
